{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in c:\\users\\hp\\appdata\\roaming\\python\\python37\\site-packages (1.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\hp\\appdata\\roaming\\python\\python37\\site-packages (from torch) (4.7.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in c:\\users\\hp\\appdata\\roaming\\python\\python37\\site-packages (4.30.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\appdata\\roaming\\python\\python37\\site-packages (from transformers) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in c:\\users\\hp\\appdata\\roaming\\python\\python37\\site-packages (from transformers) (0.16.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\program files\\python37\\lib\\site-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\appdata\\roaming\\python\\python37\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\program files\\python37\\lib\\site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\program files\\python37\\lib\\site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: requests in c:\\program files\\python37\\lib\\site-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\hp\\appdata\\roaming\\python\\python37\\site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\hp\\appdata\\roaming\\python\\python37\\site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\program files\\python37\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\hp\\appdata\\roaming\\python\\python37\\site-packages (from transformers) (6.7.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\hp\\appdata\\roaming\\python\\python37\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\hp\\appdata\\roaming\\python\\python37\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\roaming\\python\\python37\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\program files\\python37\\lib\\site-packages (from importlib-metadata->transformers) (3.15.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\program files\\python37\\lib\\site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\program files\\python37\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\program files\\python37\\lib\\site-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\program files\\python37\\lib\\site-packages (from requests->transformers) (2022.12.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch\n",
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python37\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_input = [{'question': 'What are neutrinos?',\n",
    "             'context': 'Neutrinos are elusive subatomic particles that are constantly streaming through space and matter nearly undetected. Though trillions of neutrinos pass through your body every second, they rarely interact with matter due to their neutral charge and tiny mass. Neutrinos come in three flavors - electron, muon, and tau - each associated with a corresponding lepton particle. They are produced copiously in nuclear reactions within stars, supernovae, and other high-energy astrophysical environments. Neutrinos oscillate between flavors as they travel, implying they have mass, unlike previously theorized. The detection of solar neutrinos in the 1960s provided critical evidence that nuclear fusion powers the Sun. Neutrinos provide a unique window into these energetic phenomena across the universe due to their weak interactions. But their ghostly nature also makes them challenging to study. Determining precise neutrino masses and other properties remains an active area of research in particle physics today. Though abundantly produced in nature, much about neutrinos continues to elude scientists.'},\n",
    "             {'question': 'What significance do neutrinos have on our planet Earth?',\n",
    "              'context': 'Neutrinos pass unfelt through the matter of our planet, only rarely leaving a telltale trace of their journey. Though trillions stream through every square inch of Earth each second, these ethereal particles interact so rarely that their direct influence is but a whisper. Produced in the furious furnace of the Sun core, neutrinos zip through its dense plasma unhindered, emerging straight from the heart of the nuclear forge. Detecting these solar neutrinos proved the power source of stars, yet our world remains largely oblivious to the torrent passing through it. On occasion, a neutrino will collide with an atom in our upper atmosphere, creating a shower of particles that reveals where the neutrino began its cosmic trek. And deep underground, in laboratories shielded from other radiation, scientists patiently watch for the next faint flash that announces a neutrinos arrival. While their ghostly presence flows through the rocks, water, and lives of our planet, neutrinos remain aloof, barely noticeable except by those seeking their subtle signs. The Earth itself feels no direct impact from the trillions of neutrinos traversing it every moment. These particles reveal their secrets only to those determined to listen for their whisper-soft footsteps.'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'deepset/roberta-base-squad2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading model.safetensors: 100%|██████████| 496M/496M [14:59<00:00, 552kB/s]  \n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python37\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\hp\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 79.0/79.0 [00:00<00:00, 12.0kB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 899k/899k [00:01<00:00, 667kB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 596kB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 772/772 [00:00<00:00, 103kB/s]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForQuestionAnswering(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs0 = tokenizer(QA_input[0]['question'], QA_input[0]['context'], return_tensors=\"pt\")\n",
    "outputs0 = model(**inputs0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs1 = tokenizer(QA_input[1]['question'], QA_input[1]['context'], return_tensors=\"pt\")\n",
    "outputs1 = model(**inputs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0,  2264,    32, 28979,   338, 10968,   116,     2,     2, 14563,\n",
      "          1182,   338, 10968,    32, 21483,  2849, 45826, 16710,    14,    32,\n",
      "          5861,  5230,   149,   980,     8,   948,   823, 35777, 21722,     4,\n",
      "          3791, 35245,     9, 28979,   338, 10968,  1323,   149,   110,   809,\n",
      "           358,   200,     6,    51,  7154, 10754,    19,   948,   528,     7,\n",
      "            49,  7974,  1427,     8,  5262,  2862,     4,  3864,  1182,   338,\n",
      "         10968,   283,    11,   130, 14250,   111, 35235,     6, 14701,   261,\n",
      "             6,     8,   326,  1180,   111,   349,  3059,    19,    10, 12337,\n",
      "          2084,  9216, 33100,     4,   252,    32,  2622,  9212,  9997,    11,\n",
      "          1748, 11012,   624,  2690,     6,  2422, 33278,  4791,     6,     8,\n",
      "            97,   239,    12, 18261, 38685, 45537, 11534,     4,  3864,  1182,\n",
      "           338, 10968, 19615,   877,   227, 14250,    25,    51,  1504,     6,\n",
      "         28695,    51,    33,  2862,     6,  7328,  1433, 38712,  1538,     4,\n",
      "            20, 12673,     9,  4118, 28979,   338, 10968,    11,     5,  7571,\n",
      "            29,  1286,  2008,  1283,    14,  1748, 24904,  4361,     5,  2083,\n",
      "             4,  3864,  1182,   338, 10968,   694,    10,  2216,  2931,    88,\n",
      "           209, 20425, 32242,   420,     5,  9468,   528,     7,    49,  3953,\n",
      "         11324,     4,   125,    49, 15934,   352,  2574,    67,   817,   106,\n",
      "          4087,     7,   892,     4,   211, 39938,   154, 12548, 28979,   338,\n",
      "          1696, 15444,     8,    97,  3611,  1189,    41,  2171,   443,     9,\n",
      "           557,    11, 33100, 17759,   452,     4,  3791, 38761,  2622,    11,\n",
      "          2574,     6,   203,    59, 28979,   338, 10968,  1388,     7,  1615,\n",
      "          6343,  4211,     4,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "print(inputs0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuestionAnsweringModelOutput(loss=None, start_logits=tensor([[ 1.4723e+00, -8.8066e+00, -9.0298e+00, -7.2342e+00, -9.0009e+00,\n",
      "         -9.5348e+00, -9.6535e+00, -9.6084e+00, -8.8250e+00,  3.9565e+00,\n",
      "         -2.2552e+00, -4.0154e+00, -3.4944e+00,  2.9191e+00,  7.2462e+00,\n",
      "          6.0594e+00, -1.7181e-01,  7.5685e-03, -1.6786e+00, -3.4832e+00,\n",
      "          2.7035e-01, -1.4896e+00, -4.8265e+00, -3.1090e+00, -6.2729e+00,\n",
      "         -2.5536e+00, -3.1922e+00, -2.4817e+00, -3.3697e+00, -4.1333e+00,\n",
      "         -2.9584e+00, -1.0724e+00, -7.5124e+00,  1.5822e+00, -6.4141e+00,\n",
      "         -6.1026e+00, -4.7463e+00, -6.9502e+00, -5.3589e+00, -5.6464e+00,\n",
      "         -5.9088e+00, -5.8754e+00, -7.3776e+00, -2.0794e+00, -3.3895e+00,\n",
      "         -5.2708e+00, -7.8365e+00, -4.7896e+00, -6.0100e+00, -6.6765e+00,\n",
      "         -3.0857e+00, -1.8186e+00, -4.7425e+00, -6.7252e+00, -1.9632e+00,\n",
      "         -3.6382e+00, -4.1332e+00,  9.6800e-01, -4.4473e+00, -6.4534e+00,\n",
      "         -5.8758e+00, -1.4171e+00, -4.5236e+00,  7.5589e-01, -2.0329e+00,\n",
      "         -4.3009e+00,  1.2639e-01, -7.1703e+00, -2.6868e+00, -6.8833e+00,\n",
      "         -7.1772e+00, -7.2503e+00, -4.3887e+00, -5.4695e+00, -4.7415e+00,\n",
      "         -3.6765e+00, -4.6083e+00, -7.1589e+00, -4.9187e+00, -3.9678e+00,\n",
      "         -2.6661e+00, -6.1922e+00, -4.3809e+00, -4.1332e+00,  7.3364e-01,\n",
      "         -2.7607e+00, -9.3694e-01, -1.4848e+00, -4.7739e+00, -3.2349e+00,\n",
      "         -8.8469e-01, -4.0601e+00, -4.2558e+00, -2.1264e+00, -7.1960e+00,\n",
      "         -2.3462e+00, -6.1857e+00, -6.5685e+00, -7.5577e+00, -6.9201e+00,\n",
      "         -5.3541e+00, -3.7712e-01, -6.1932e+00, -3.3600e+00, -2.8900e+00,\n",
      "         -5.7450e+00, -4.5897e+00, -4.1334e+00, -1.0213e-01, -5.2274e+00,\n",
      "         -7.0896e+00, -6.5829e+00, -2.1796e+00, -7.8609e+00, -5.1705e+00,\n",
      "         -4.4813e+00, -5.3907e+00, -5.6623e+00, -4.7753e+00, -7.5182e+00,\n",
      "         -3.4317e+00, -2.6554e+00, -4.5002e+00, -1.9539e+00, -8.2835e+00,\n",
      "         -5.6946e+00, -6.1045e+00, -4.4543e+00, -6.8150e+00, -4.1333e+00,\n",
      "         -4.9805e+00, -5.8476e+00, -8.3240e+00, -3.7432e+00, -4.3232e+00,\n",
      "         -8.4125e+00, -7.9032e+00, -8.3707e+00, -7.9524e+00, -6.9223e+00,\n",
      "         -9.0148e+00, -7.7101e+00, -7.0351e+00, -7.4774e+00, -8.0130e+00,\n",
      "         -4.0188e+00, -6.7645e+00, -7.3929e+00, -6.9857e+00, -5.9979e+00,\n",
      "         -4.1333e+00, -1.6681e+00, -6.8560e+00, -7.9646e+00, -7.5561e+00,\n",
      "         -5.6506e+00, -5.1380e+00, -4.3000e+00, -5.5753e+00, -6.3150e+00,\n",
      "         -5.3522e+00, -1.7947e+00, -4.5505e+00, -6.2722e+00, -7.0844e+00,\n",
      "         -6.2293e+00, -7.4313e+00, -8.1468e+00, -5.8022e+00, -4.2211e+00,\n",
      "         -5.7381e+00, -4.1332e+00, -8.1848e+00, -4.1647e+00, -4.0338e+00,\n",
      "         -8.1724e+00, -7.1577e+00, -9.1474e+00, -7.9759e+00, -7.8087e+00,\n",
      "         -5.2874e+00, -8.3545e+00, -6.9780e+00, -9.4784e+00, -4.8441e+00,\n",
      "         -7.4141e+00, -8.2668e+00, -5.9580e+00, -3.5752e+00, -8.2475e+00,\n",
      "         -8.3518e+00, -6.4311e+00, -9.3199e+00, -8.6003e+00, -8.0136e+00,\n",
      "         -8.6316e+00, -8.2208e+00, -7.9040e+00, -8.7765e+00, -8.9238e+00,\n",
      "         -7.3151e+00, -8.7163e+00, -4.3987e+00, -8.2094e+00, -8.6057e+00,\n",
      "         -4.1333e+00, -6.0784e+00, -4.2768e+00, -5.7227e+00, -7.3521e+00,\n",
      "         -6.4718e+00, -9.5027e+00, -5.0635e+00, -8.3020e+00, -4.8144e+00,\n",
      "         -8.7230e+00, -8.5948e+00, -7.0792e+00, -8.3526e+00, -5.0413e+00,\n",
      "         -7.9614e+00, -7.1808e+00, -4.1332e+00, -9.4782e+00]],\n",
      "       grad_fn=<CloneBackward0>), end_logits=tensor([[ 1.6740, -8.2453, -8.4234, -9.1969, -8.3126, -6.9344, -6.3164, -6.9878,\n",
      "         -4.3426, -4.0742, -6.0076, -5.4476, -1.1268, -2.0375,  2.4949, -1.3928,\n",
      "         -1.1230,  7.6542,  0.9915, -3.0673, -2.5412, -2.5854, -4.8503,  0.6826,\n",
      "         -5.6548,  0.7643, -5.1062, -2.5838,  4.5564,  1.5863, -6.0895, -2.7114,\n",
      "         -6.0588, -5.3649, -5.9486, -0.8690, -7.3300, -7.1520, -7.0760, -2.7159,\n",
      "         -6.9677, -2.3710, -3.3145, -6.2591, -7.3088, -7.6199, -7.7466, -1.8627,\n",
      "         -7.0369, -7.8327, -7.0340, -5.8634, -3.8375, -7.6472, -5.8358, -0.5519,\n",
      "          1.5863, -5.9685, -7.6781, -7.1082, -3.1376, -6.0604, -6.1443, -2.1545,\n",
      "          1.2097, -3.8626, -2.6649, -5.9378, -5.0364, -3.2256, -5.3046, -5.9542,\n",
      "         -5.2326,  1.2273, -0.2692, -4.6394, -5.9226, -6.9495, -7.2479, -6.5147,\n",
      "         -6.6885, -4.5209,  0.9634,  1.5863, -4.6264, -6.9896, -3.1963, -6.8297,\n",
      "         -2.6980, -6.5978, -4.3580, -0.8261, -5.9009, -1.0592, -6.1731, -6.5067,\n",
      "         -5.4729, -0.9111, -4.6673, -6.3423, -6.2495, -5.9710, -7.1292, -2.7686,\n",
      "         -4.5628, -2.7080,  1.4931,  1.5861, -6.6155, -8.0765, -7.2622, -3.0544,\n",
      "         -7.1488, -5.2665, -7.0387, -1.9399, -7.6374, -7.4817, -1.6002, -3.8820,\n",
      "         -7.4885, -7.5103, -8.0404, -1.1385, -3.4426, -7.2959, -8.1665, -6.8636,\n",
      "         -1.7257,  1.5862, -8.9516, -8.1071, -8.6118, -5.8746, -7.1106, -6.9110,\n",
      "         -2.6835, -8.7968, -9.0552, -8.4064, -5.6139, -8.6310, -8.8504, -7.9845,\n",
      "         -8.8874, -7.3226, -6.2569, -7.8726, -8.4746, -3.8177,  1.5862, -7.3034,\n",
      "         -8.4296, -7.4075, -3.4513, -8.2977, -8.2995, -7.2266, -6.7851, -7.7807,\n",
      "         -7.9241, -4.4307, -1.9293, -7.5859, -8.0775, -2.6689, -7.8239, -8.5013,\n",
      "         -8.4911, -7.0804, -3.0636,  1.5863, -8.0498, -8.5006, -7.6824, -6.7554,\n",
      "         -3.7177, -8.1111, -8.9988, -7.8962, -7.9120, -8.2010, -3.9496, -4.7588,\n",
      "         -9.0806, -9.1628, -8.6472, -8.3885, -8.4493, -7.9959, -6.0042, -4.6527,\n",
      "         -8.4035, -8.8550, -5.6448, -8.6883, -9.2276, -9.0770, -8.1406, -8.8239,\n",
      "         -6.3562, -8.7567, -7.2759, -4.6488, -5.4589,  1.5862, -8.9645, -7.1656,\n",
      "         -5.9742, -8.7396, -3.2514, -6.5250, -8.0900, -8.7420, -8.9466, -8.1269,\n",
      "         -4.6723, -8.9592, -8.6924, -8.2670, -7.8276, -3.0509,  1.5863, -7.6653]],\n",
      "       grad_fn=<CloneBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "print(outputs0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0,  2264, 11382,   109, 28979,   338, 10968,    33,    15,    84,\n",
      "          5518,  3875,   116,     2,     2, 14563,  1182,   338, 10968,  1323,\n",
      "          9515,  6607,   149,     5,   948,     9,    84,  5518,     6,   129,\n",
      "          7154,  1618,    10,  1137, 31029, 13946,     9,    49,  3251,     4,\n",
      "          3791, 35245,  4615,   149,   358,  3925, 10468,     9,  3875,   349,\n",
      "           200,     6,   209,   364, 38537, 16710, 10754,    98,  7154,    14,\n",
      "            49,  2228,  2712,    16,    53,    10, 37539,     4, 21270,  7618,\n",
      "            11,     5, 15940, 36574,     9,     5,  2083,  2731,     6, 28979,\n",
      "           338, 10968, 23595,   149,    63, 19790, 29051, 24376,  2028,  3215,\n",
      "             6,  3947,  1359,    31,     5,  1144,     9,     5,  1748, 21733,\n",
      "             4, 18129,   154,   209,  4118, 28979,   338, 10968,  4362,     5,\n",
      "           476,  1300,     9,  2690,     6,   648,    84,   232,  1189,  2743,\n",
      "         35606,     7,     5, 23489,  3133,   149,    24,     4,   374,  5852,\n",
      "             6,    10, 28979,   338,  1696,    40, 36191,    19,    41, 37113,\n",
      "            11,    84,  2853,  5466,     6,  2351,    10,  9310,     9, 16710,\n",
      "            14,  7441,   147,     5, 28979,   338,  1696,   880,    63, 30837,\n",
      "         18526,     4,   178,  1844,  9111,     6,    11, 29007, 33855,    31,\n",
      "            97, 13785,     6,  4211, 30727,  1183,    13,     5,   220, 27922,\n",
      "          7462,    14, 13835,    10, 28979,   338, 10968,  5237,     4,   616,\n",
      "            49, 15934,   352,  2621,  7964,   149,     5, 10889,     6,   514,\n",
      "             6,     8,  1074,     9,    84,  5518,     6, 28979,   338, 10968,\n",
      "          1091,  1076, 37298,     6,  6254, 20228,  4682,    30,   167,  1818,\n",
      "            49, 12405,  2434,     4,    20,  3875,  1495,  2653,   117,  2228,\n",
      "           913,    31,     5, 35245,     9, 28979,   338, 10968, 34038,   154,\n",
      "            24,   358,  1151,     4,  1216, 16710,  4991,    49, 12200,   129,\n",
      "             7,   167,  3030,     7,  4161,    13,    49, 37539,    12, 24810,\n",
      "         18424,     4,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "print(inputs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuestionAnsweringModelOutput(loss=None, start_logits=tensor([[ 0.4277, -8.7774, -8.7048, -9.5244, -8.9525, -9.5992, -9.8416, -9.3120,\n",
      "         -9.2991, -9.2509, -9.2578, -9.0182, -9.6370, -9.6051, -9.5608, -1.5139,\n",
      "         -7.2840, -7.9501, -7.4700, -1.5958, -1.7819, -5.2673, -4.2107, -3.4012,\n",
      "         -3.2218, -6.5636, -3.5280, -5.4585, -4.5591,  0.2371,  0.4778, -1.4488,\n",
      "         -2.0211, -1.7299, -4.9705, -3.3585, -5.7423, -3.6064, -3.1845, -5.9949,\n",
      "         -1.1692, -0.0973, -3.5303, -5.0631, -2.8597, -3.9394, -5.1363, -7.1179,\n",
      "         -4.4986, -4.7981, -4.1791, -5.4629,  0.3480, -0.2870, -4.9225, -3.0565,\n",
      "         -1.5830, -1.8030, -1.6004, -4.0085,  1.4384,  0.4002, -0.6337, -3.8318,\n",
      "         -0.3176,  0.7963, -0.1918, -5.9950, -1.0314, -6.3936, -4.4665, -3.6348,\n",
      "         -2.6588, -3.8301, -6.9485, -3.2699, -2.6718, -4.4666, -5.1471, -1.7432,\n",
      "         -8.1392, -7.5870, -2.8160, -5.1246, -4.3025, -3.1160, -3.5583, -3.3725,\n",
      "         -7.1868, -6.2714, -6.1094, -2.8358, -3.8080, -5.4353, -3.5680, -4.1095,\n",
      "         -6.9463, -3.6250, -2.3044, -4.6562, -7.4566,  0.0597, -4.7813, -3.9974,\n",
      "         -2.1799, -2.7010, -8.0781, -7.0142, -0.1576,  0.4125,  0.7326, -3.1884,\n",
      "         -5.4482, -2.2300, -7.5298, -4.1405, -0.5628, -4.9265, -2.5780, -1.9383,\n",
      "         -2.2268, -6.5385, -4.4365, -3.1003, -5.7506, -7.6076, -6.0130, -5.9947,\n",
      "         -2.4619, -5.2320, -8.0161, -2.7155, -3.4724, -8.5361, -8.2268, -5.8326,\n",
      "         -3.4793, -7.2768, -4.9199, -5.0741, -6.7659, -4.3387, -4.5713, -5.6052,\n",
      "         -7.3031, -3.8018, -3.2298, -3.2898, -7.7723, -4.1793, -6.0124, -3.5473,\n",
      "         -4.0847, -4.1956, -3.6419, -8.2269, -8.0417, -4.8167, -5.7520, -2.5283,\n",
      "         -5.2138, -5.9947, -8.2797, -4.3521, -7.3829, -8.7924, -6.0521, -5.6956,\n",
      "         -5.9344, -8.3198, -6.0798, -4.8406, -8.9019, -3.8363, -6.1800, -6.0977,\n",
      "         -7.0991, -5.7007, -5.5069, -4.2124, -6.3865, -8.5521, -6.3004, -6.5947,\n",
      "         -3.7974, -8.8302, -8.8257, -6.1778, -5.9947,  0.0770,  1.0586,  2.0550,\n",
      "         -4.4862, -1.9060, -1.7704, -3.6922, -1.8740, -1.7928, -7.2310, -2.7750,\n",
      "         -7.5701, -6.8337, -3.5856, -7.1975, -3.7807, -5.6408, -6.2159, -0.5330,\n",
      "         -7.6729, -7.2974, -3.3079, -2.0312, -5.8776, -6.6875, -1.9691, -3.7183,\n",
      "         -6.4761, -6.3501, -4.7145, -4.4354, -3.8360, -2.9699, -4.9822, -7.1558,\n",
      "          0.9428, -1.6449, -2.9402, -1.6082, -1.1115, -3.0441, -3.9019, -5.7983,\n",
      "         -4.1546, -1.7827, -8.2332, -2.7894, -8.4207, -7.2459, -4.5427, -8.3513,\n",
      "         -7.2630, -5.4143, -6.1234, -5.9948,  0.3286, -1.9873, -0.4913, -1.0641,\n",
      "         -0.0424, -2.8779, -2.8228, -3.2747, -4.1597, -5.3503, -3.9625, -6.2299,\n",
      "         -3.0505, -2.0387, -6.7808, -4.7567, -3.2492, -5.9948, -9.3343]],\n",
      "       grad_fn=<CloneBackward0>), end_logits=tensor([[ 0.8583, -8.5130, -7.8811, -8.2190, -8.9633, -8.3723, -7.8839, -8.3955,\n",
      "         -8.5355, -8.5800, -8.2565, -7.4672, -7.7234, -7.7740, -7.1537, -7.2642,\n",
      "         -8.1202, -7.3407, -4.2117, -5.9340, -5.3607, -0.7340, -4.9282, -6.5607,\n",
      "         -1.2599, -5.4492, -6.2532, -1.0241, -3.1433, -4.9688, -3.3391, -4.3771,\n",
      "         -5.8415, -5.7461, -2.6085,  0.4193, -5.4696, -4.9538,  1.5226, -0.5940,\n",
      "         -4.8031, -1.1132, -5.5462, -5.5611, -6.1574, -5.7927, -1.4684, -5.7805,\n",
      "         -1.2219, -5.8484, -0.6164, -2.0692, -5.8258, -6.3280, -3.3794, -1.6847,\n",
      "         -2.5165, -6.4095, -1.1417, -5.7455, -5.3454, -4.1985,  0.6377, -6.0006,\n",
      "         -6.2035, -5.6954,  2.7239, -0.5942, -5.7299, -5.0191, -7.0486, -7.0438,\n",
      "         -5.7698, -4.1374, -7.0031, -7.2182, -5.0794, -1.2091, -4.1894, -7.3526,\n",
      "         -7.6062, -3.6900, -6.3715, -6.8310, -7.0417, -6.0647, -2.0501, -6.2000,\n",
      "         -5.7353, -1.0734, -3.7667, -5.7720, -7.2770, -7.3958, -7.2786, -5.5719,\n",
      "         -7.3539, -7.0742, -4.9069, -1.0189, -1.6363, -5.7495, -6.1240, -6.4682,\n",
      "         -5.2154, -7.2916, -7.0638, -2.7471, -5.9867, -6.1091, -4.1247, -1.9317,\n",
      "         -6.0855,  1.3642, -1.9163, -4.9942, -7.1337, -5.5605, -7.0262, -6.6699,\n",
      "         -3.2809, -6.1232, -6.7077, -1.6977, -6.0764, -5.7873,  0.2557, -0.5936,\n",
      "         -7.6887, -5.6111, -6.8795, -7.7337, -8.0920, -7.8192, -4.4047, -7.5548,\n",
      "         -6.0319, -7.5311, -7.9292, -3.3188, -7.9457, -7.9441, -7.2754, -2.0096,\n",
      "         -4.2817, -6.9809, -7.6684, -3.9854, -7.2538, -2.6258, -7.3182, -5.4964,\n",
      "         -7.3310, -8.1361, -8.0020, -7.6138, -4.2010, -5.6773, -7.0843, -4.1631,\n",
      "         -0.9077, -0.5936, -7.4318, -7.9787, -4.6400, -7.0705, -8.9973, -5.7180,\n",
      "         -7.3258, -8.3454, -8.3508, -3.2045, -5.6658, -5.3494, -7.6529, -7.4387,\n",
      "         -8.1018, -8.4639, -7.7147, -6.0105, -4.7144, -7.9303, -7.5645, -8.5000,\n",
      "         -8.1343, -7.5618, -4.8657, -3.7662, -0.5936, -6.6434, -5.3225, -2.9482,\n",
      "          0.4614,  1.2194, -4.8425, -5.7559, -6.9390, -2.5648, -6.9893, -1.1465,\n",
      "         -5.7395, -6.6425, -0.5544, -5.5316, -6.2524, -0.0968, -3.2465, -6.8664,\n",
      "         -7.2696, -2.4423, -6.9116, -6.6817, -1.3213, -4.1626, -5.8925, -1.9500,\n",
      "         -6.8895, -8.0598, -6.7964, -7.2193, -7.3126, -5.9294, -1.2497, -2.4090,\n",
      "         -6.2601, -3.9755, -4.7719, -5.1717, -3.9766, -4.0551,  0.2522, -5.5102,\n",
      "         -7.5106, -2.5598, -6.3681, -7.3212, -6.3734, -1.2755, -7.2326, -5.9754,\n",
      "         -2.1782, -6.2303, -0.2527, -0.5938, -5.9549, -3.9918, -6.2753, -6.2444,\n",
      "          1.3369, -4.1107, -6.7136, -5.2581, -5.9537, -7.0669, -5.3281, -6.9744,\n",
      "         -6.4559, -3.9329, -6.8949, -4.7629,  0.3505, -0.5937, -7.6325]],\n",
      "       grad_fn=<CloneBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "print(outputs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ques: What are neutrinos?\n",
      "answer:  elusive subatomic particles\n"
     ]
    }
   ],
   "source": [
    "answer_start_idx = torch.argmax(outputs0.start_logits)\n",
    "answer_end_idx = torch.argmax(outputs0.end_logits)\n",
    "\n",
    "answer_tokens = inputs0.input_ids[0, answer_start_idx: answer_end_idx + 1]\n",
    "answer = tokenizer.decode(answer_tokens)\n",
    "print(\"ques: {}\\nanswer: {}\".format(QA_input[0]['question'], answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ques: What significance do neutrinos have on our planet Earth?\n",
      "answer: \n"
     ]
    }
   ],
   "source": [
    "answer_start_idx = torch.argmax(outputs1.start_logits)\n",
    "answer_end_idx = torch.argmax(outputs1.end_logits)\n",
    "\n",
    "answer_tokens = inputs1.input_ids[0, answer_start_idx: answer_end_idx + 1]\n",
    "answer = tokenizer.decode(answer_tokens)\n",
    "print(\"ques: {}\\nanswer: {}\".format(QA_input[1]['question'], answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = pipeline('question-answering', model=model_name, tokenizer=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.pipelines.question_answering.QuestionAnsweringPipeline at 0x1331e387d30>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.668811023235321, 'start': 14, 'end': 41, 'answer': 'elusive subatomic particles'}\n"
     ]
    }
   ],
   "source": [
    "output_0 = qa(QA_input[0]['question'], QA_input[0]['context'])\n",
    "print(output_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.02256554178893566, 'start': 232, 'end': 271, 'answer': 'their direct influence is but a whisper'}\n"
     ]
    }
   ],
   "source": [
    "output_1 = qa(QA_input[1]['question'], QA_input[1]['context'])\n",
    "print(output_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
